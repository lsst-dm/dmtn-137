\section{Background}

In April 2019, LSST DM began a proof of concept project with the AWS and HTCondor teams to explore whether a cloud deployment of the Data Release Production (DRP) is feasible.
The execution plan of the project is described in DMTN-114.
For this project AWS granted us credits to use their platform.
The team met biweekly to discuss the progress and plans.
In this document we report the results.


\section{Approaches and Strategies}

In executing this PoC we focused on the goal of being able to demonstrate the data processing on the AWS cloud and kept the following strategies in mind.

\begin{enumerate}
\item
Progress in phases.
Following the ideas in DMTN-114 we started with simply moving the execution to a cloud instance without modifying any backend, and gradually switching out each component to use AWS products.
Amazon's Network File System, Amazon Elastic File System (Amazon EFS), was used in the first phase as the storage of a shared data repository, before the S3 datastore could be used directly via the Data Butler interface.
A sqlite file on Amazon EFS was as the Data Butler Registry before we moved it to an Amazon RDS instance.
A Personal Condor Pool on one instance was used before we tested the capability to launch new instances as the Condor workers.
This approach allowed us to debug and adapt more smoothly and always had a fallback option in integrating and testing new features and components.
\item
Test with small tasks and dataset before scaling up.
In particular, the ci\_hsc \href{https://github.com/lsst/ci\_hsc} dataset and the CI workflow provide a minimal HSC test dataset and a representative DRP workflow and algorithms.
The test workflow was always run as one of the first integration steps.
\item
Use Gen3 middleware.
One of the mandates in the PoC was to use the Gen3 Middleware, which is designed to ease the DRP execution and automation compared to the previous Gen2 Middleware.
However, as the Gen3 Middleware has been under active development during the course of the PoC, there were many backwards incompatible changes, the API and registry schema were unstable, no Gen3 continuous integration tests existed when we first started, and the test coverage was not good.
We did not always follow the bleeding edge version and updated only when unnecessary, such as important bug fixes.
Nonetheless the stack versions should not have strong impacts in the PoC conclusions.
\item
Focus on end-to-end execution and leave optimization behind.
Some potential optimizations and further investigations were identified throughout the PoC project but were not carried out.
Ideas are described in Sec \ref{future}.
\end{enumerate}


\section{Architecture Design}

The system design in the end of the PoC is as in the diagram in Figure~\ref{fig:arch}.
All components are hosted on the AWS platform.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/arch}
  \label{fig:arch}
  \caption{architecture design}
\end{figure}

The user launches an on-demand instance as the master submit host and the central manager of the HTCondor.
HTCondor manages an HTCondor pool which is a collection of resources.
On the master, the command line interface of Pegasus is used to submit, control, and monitor the LSST workflow.
Each LSST Pipeline Task quantum is mapped into a Pegasus job in the abstract workflow which is resource-independent.
The job dependency is represented as a directed acyclic graph (DAG).
Pegasus adds other necessary jobs, such as data transfer, to the executable workflow.
HTCondor DAGMan is the workflow execution engine behind Pegasus and controls the processes.
HTCondor match jobs with available resources.

Using HTCondor Annex, AWS EC2 instances are launched as workers to join the HTCondor Pool.
Either the on-demand or the Spot instances can be added.
Fleets of multiple Spot instances can be requested using HTCondor Annex commands.
HTCondor Annex manages the instance lifecycle.

LSST Pipeline jobs are executed on the worker instances.
Science files are stored in a S3 bucket which is a Data Butler Datastore, and pipelines jobs read and write directly to the S3 bucket.
Pipeline jobs also connect directly to the RDS instance which is the Data Butler Registry that keeps track of the science files.
These new Data Butler backends were implemented during the PoC, see Sect \ref{butler}.

Files that are not managed by Data Butler are managed by the Condor File IO via Pegasus.
These include the Data Butler configuration file, pipeline definition (Quantum Graph) files, and the log files.
The master instance also serves as the staging site for these files.
Condor File IO transfers input and output files between the master and the workers; instances do not share a filesystem.


\subsection{Alternative architecture designs were also discussed in the PoC}

We discussed different architecture designs but did not pursue all of them due to time constraints.
One prominent idea is to use condor transfer to read and write files to S3, rather than relying on Data Butler to communicate directly with S3.
A new plug-in to do so has been recently added in HTCondor.
In this design HTCondor controls all file transfer.

Similarly, database ingestion may be done only after the jobs fully finish, rather than one at a time while the jobs run.
This also may reduce the number of database connections by bundling transactions from multiple jobs together.
The idea here is to handle database transactions in a centralized manner, not on workers, so can have a limited number of database connections.
This design may require a local sqlite registry to be used in each job.
Job wrappers and new Data Butler features may be needed.


\section{Building AWS support into the Data Butler and lessons learned}

The Data Butler is the overarching data IO abstraction through which all LSST data access is mediated. Datasets are referred to by their unique IDs, or a set of identifying references, which are then resolved through an registry that matches the dataset IDs, or references, to the location, file format and the Python object type of the dataset. The system that persists, reads and potentially modifies the datasets is called the datastore. The Registry almost always backed by an SQL database and the Datastore is usually backed by a shared filesystem. A major focus of AWS POC was to implement, and investigate issues related to, an S3 backed Datastore and a PostgreSQL backed Registry.

Simple Storage Service (S3) is object storage provided by AWS. Unlike the more traditional file systems that manage data as a file hierarchy, or data blocks within sectors and tracts, objet storage manages data as objects. This allows the storage of massive amounts of unstructured data where each object typically includes the data, related metadata and is identified by a globally unique identifier. S3, specifically, promises 99.999999999\% durability as uploading an object to it automatically stores it across multiple systems, thus also ensuring scalability and performance. Related objects are generally stored in the same Buckets for easier administrative purposes. Access, read, write, delete and other atomical units of action on the objects themselves can be allowed or forbidden at the account, bucket or individual object level. Logging is available for all actions on the Bucket level and/or at the individual object granularity. It is also possible to define and issue complex alert conditions on Bucket or object actions which can execute arbitrary actions or workflows.

PostgreSQL is one of the most popular open source relational database systems available. The choice to go with PostgreSQL was based on the fact that it's a very popular and well supported open source software that suffers from no additional licensing fees usually associated with proprietary software. Relational Database Service (RDS) is the AWS cloud service that launches and configures databases with ease. 

At the time the AWS POC Group began generation 3 Butler, the latest implementation of the Data Butler, implemented PosixDatastore, a local or shared filesystem datastore, and a SqliteRegistry. OracleRegistry followed soon after LSST AWS POC group began work. Initialy the focus was on implementing an S3 backed datastore called S3Datastore. The interface between AWS services and LSST Stack would be based on the official AWS SDK called boto3. In March 2019, Dino Bektesevic visited NOAO to work more closely with Tim Jenness, which prooved to be instrumental in implementing the early versions of a new module in the `daf\_butler` called `s3utils`, an `S3Datastore` class, the PosixDatastore equivalent, and a set of appropriate unit tests that demonstrated its functionality and correctness. The unit tests for the datastore utilize the `moto` library which mocks requests and responses sent to AWS services, so that no additional external infrastructure is required to use it. PostgreSqlRegistry class was implemented partially during the visit and completed shortly after the visit. The initial implementation showcasing the required changes to the code was submitted as a Draft Pull Request \href{https://github.com/lsst/daf_butler/pull/147}{PR-147}. 

The tentative implementation revealed issues with how the Data Butler treated Uniform Resource Identifiers, or URIs, which were, at the time, not being handled correctly, as per standards defined in \href{https://tools.ietf.org/html/rfc3986}{FC-3986}, by The Location class. After expansive discussions and an example re-implementation called S3Location to demonstrate the issues, in May 2019 Tim Jenness authored the `ButlerURI` class (\href{https://github.com/lsst/daf_butler/pull/167}{PR-167}) resolving the issues. Major efforts were then invested into refining the newly added code to the level of production quality as well as updating the remaining Gen. 3 Butler to use the updated ButlerURI code instead. Every call to OS functionality had to be generalized to take a URI and from it determine the appropriate operation - a call to OS functionality, a AWS operation or something else. This led changes in Butler, Config, ButlerConfig and YAML Loader classes. These changes made the whole of Data Butler more general and pliable to future changes, such as adding support for other cloud providers. 

Further integration of the S3 backend required a change to Formatter classes to enable data serialization and deserialization to and from bytes. Formatters present interfaces for reading and writing of Python objects to and from files. They are the mechanism underlying how Data Butler is capable of presenting data as science products in the form of Python objects, abstracting away the underlying file types. Modifications were made to JsonFormatter, PickeFormatter, YamlFormatter, PexConfigFormatter and the generic abstract class Formatter. This concluded the last of changes required for S3Datastore integration. After which Jenkins integration tests were run and the S3Datastore and supplemental code was merged to master branch of the `daf\_butler` repository in \href{https://github.com/lsst/daf_butler/pull/179}{PR-179} (the associated Jira ticket is \href{https://jira.lsstcorp.org/browse/DM-13361}{DM-13361}). It became apparent that there are certain similarities that are shared between PosixDatastore and S3Datastore, similarities that would be shared by other future datastore implentations. To reduce code duplication the general datastore code was refactored and reorganized in \href{https://github.com/lsst/daf_butler/pull/187}{PR-187} shortly after. 

PostgreSqlRegistry was not part of this PR. The initial implementation was based on OracleRegistry, due to the similarities between the two, but was re-implemented in terms of the generic SqlRegistry class in July. Problems were caused, for both Oracle and PostgreSQL, by the table naming conventions and additionally, for PostgreSQL, the table views did not conform to the assumptions made. In July the PostgreSqlRegistry was re-implemented in terms of the more general SqlRegistry and a new SQLAlchemy expressions compiler was written, so that table views could be generated correctly. The policy for additional registry implementations was not to accept associated unit tests, as they are dependent on existing outside architecture, meant that checking wheter it worked or not had to be based on manually executing one of the continuous integration tests such as ci\_hsc. I migrated existing SQLite registries to PostgreSQL in July and August and made them available to the LSST AWS PoC group for testing. The code was merged into the master branch of Gen. 3 Butler in August with \href{https://github.com/lsst/daf_butler/pull/161}{PR-161}. A major issue was then discovered when issuing rollback statements during error recovery stemming from assumptions made when implementing how all of the current SQL registries handle errors during transactions. A stopgap solution, that works for all currently implemented registries, was implemented in \href{https://github.com/lsst/daf_butler/pull/190}{PR-190} and a more complete solution was then implemented by Andy Salnikov in \href{https://github.com/lsst/daf_butler/pull/196}{PR-196}.

Outstanding issues are presented in terms of security and authorization when dealing with both S3Datastore and PostgreSqlRegisty, with PostgreSqlRegisty being especially sensitive to these issues. Security has received the outmost attention by the LSST AWS PoC group. Significant attention was paid to preserving the flexibility of the authentication in order to be able to incorporate external authenticators such as Oracle Wallets and AWS IAM Roles and Policies. There were several different iterations and improvements made to the authentication implementation (\href{https://github.com/lsst/daf_butler/pull/189}{PR-189}, \href{https://github.com/lsst/daf_butler/pull/180}{PR-180} and \href{https://github.com/lsst/daf_butler/pull/191}{PR-191}) that resulted with the current implementation. An older Gen. 2 Butler module, `db\_auth`, was re-implemented in Python by Kian-Tat Lim and added to Gen. 3 Butler so that the module would support basic file based authentication in absence of external authentication methods. Additional layers of security are achieved through EC2/S3/RDS interfaces by IP white/blacklisting , IAM, Policies etc. These policies can be very granular, affecting individually selected objects, Bucket-wide to placing all instances on the same, externally innaccessible, Virtual Private Network (VPN).

Adding the support for AWS into the Butler exercised almost the entirety of the Gen. 3 Data Butler. During the process many faults and unpredictable behaviors were discovered and solved. Many problems touched, and continue to exercise, the general Gen. 3 Data Butler implementation, as well as assumptions made during their implementation. Recounting the wide list of major improvements to the codebase, hopefully, reveals how productive this exercise has been in helping generalizing and strengthening the whole Gen3. Data Butler codebase.

\section{Results of the tract-sized runs}
\input{taskBreakdown}
\input{billBreakdown}

\section{Potential Improvements and more lessons learned} \label{future}

In this session we describe issues we have encountered during the execution and ideas to improve.
We discuss both intermittent failures that we understand and expect to occasionally encounter even in production, as well as higher level design or tooling improvements.

Failures can occur due to non-pipeline issues such as underlying infrastructure.
The fault rate may be small but as we scale up we start to encounter some.
Some examples are listed below; most seem transient.


\begin{enumerate}
\item Database connection timeout. Attempting to connect to the RDS instance failed.
\begin{lstlisting}[breaklines=true]
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection timed out
\end{lstlisting}
\item
After a file was added to a S3 bucket and during ingestion in the Butler registry, S3 reported a file does not exist.
This will be fixed in \jira{DM-22201}.
\begin{lstlisting}[breaklines=true]
FileNotFoundError: File at 's3://hsc-rc2-test1-hfc/repo-w38/hfc30/srcMatchFull/1316/srcMatchFull_1316_24_HSC_17.fits' does not exist; note that paths to ingest are assumed to be relative to self.root unless they are absolute.
\end{lstlisting}
\item S3 read timeout before science processing started in a job.
\begin{lstlisting}[breaklines=true]
botocore.exceptions.ReadTimeoutError: Read timeout on endpoint URL: "None"
\end{lstlisting}
\item Out-of-memory while running jobs.
For the same pipeline and input data, this is reproducible.
But we may not always have accurate memory usage prediction for any input data before running the jobs.
We can configure HTCondor to increase the memory requirement in the retry.
However sometimes OOM crashed the instances and appeared as a network issue, an undesirable behaviour.
\item Launching HTCondor Annex workers failed with connectivity check collector issues.
\begin{lstlisting}[breaklines=true]
Connectivity check found wrong collector (f5fc15573ffb9c93 vs a006066e73c412da).
\end{lstlisting}
\end{enumerate}


More generally, improvements in system design and tooling rise to prominence.
We discuss some ideas below.

\begin{enumerate}
\item Better job failure recovery strategies.
Our jobs write directly to the S3 bucket and the RDS instance via the Data Butler.
If a job fails in a state that partial outputs are written but the job does not fully finish, recovery is not trivial.
We may want a wrapper around the Pipeline Task that commits a transaction only if the job succeeds, have a reliable way to overwrite files from a failed job, or other ways to handle such scenarios.
\item Container based software stack.
We have found it tricky to handle the LSST stack installation, dependencies, and environments to be used together with other software.
One possible way to avoid the headaches is to use docker based stack releases.
This may also ensure consistency of software on the master and the workers more easily.
\item Better cluster management tooling.
Our current operational approach requires manually deploy suitable types and sizes of fleets based on our understanding of the overall workload.
Strategies on the instance choices and timing of requests therefore affect the cost, and manual adjustments are usually needed to reduce cost.
Annex can remove idle instances out of the condor pool but may not terminate the instances until the lease expires.
Also once instances drop out of the pool they can't be added back easily.
Tooling become essential for the operations.
For example we may use scripts to auto-scale the Annex condor pool.
An alternative approach is to use AWS Batch which automatically provisions the compute resources based on the workload and only charges for the actual resources used.
\item End-to-end CI.
This should include all operational components to do an end-to-end run.
This includes Butler repo generation, registry generation, Quantum Graph generation, job composition, workflow translation, job execution, and so on.
Many of the components were in the development phase and workarounds were used during the PoC.
For example a native Gen3 ingestion was not available so a Gen2-to-Gen3 conversion was needed.
As we put together the pieces, the absence of to do so in an automatic fashion became a key burden.
In retrospect we probably should have invested more time to automate the end-to-end workflow, even with workarounds.
Looking ahead a first step could be a \texttt{ci\_hsc} like package with the S3 backend.
\item Credential handling.
In this PoC the worker image carry credentials in files \texttt{~/.lsst/db-auth.yaml} and \texttt{~/.aws/credentials} which is not the best practice of handling the access.
\item Robustify and give better error messages.
It has been observed that sometimes the error messages could be misleading.

\end{enumerate}
